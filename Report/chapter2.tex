\chapter{مفاهیم تئوری}
در این بخش قصد داریم در مورد مفاهیم تئوری که در روش‌های مورد بررسی این پروژه استفاده شده‌اند بپردازیم.


\section{دسته‌بندی روش‌های انتخاب ویژگی}
روش‌های انتخاب ویژگی به چندین دسته تقسیم می‌شوند. دو روش متداول و شناخته‌شده‌تر آن روش‌های فیلتر\LTRfootnote{Filter} و پوشاننده\LTRfootnote{Wrapper} هستند. در روش‌های پوشاننده مستقیما ویژگی‌های انتخاب‌شده را در یک مسئله واقعی که در اینجا یک مسئله دسته‌بندی متن است استفاده می‌کنند و لذا امتیازی که برای یک مجموعه ویژگی انتخاب‌شده بدست می‌آید امتیاز دقت واقعی برای مسئله دسته‌بندی است. در مقابل و در روش‌های فیلتر،‌ با اعمال روش‌های آماری سعی می‌شود که یک امتیاز برای یک مجموعه ویژگی انتخاب‌شده حاصل گردد.
\\

روش‌های پوشاننده چون به صورت مستقیم مجموعه ویژگی را بررسی می‌کند منجر به خروجی دقیق‌تری می‌شود؛ اما باید توجه داشت که روش‌های فیلتر زمان اجرای به مراتب بهتری دارند و بر روی مسئله دسته‌بندی بایاس نخواهند شد\cite{labani2018novel}. در مسائل دسته‌بندی چون تعداد ویژگی‌ها بسیار بالاست نمی‌توان از روش‌های پوشاننده مستقیم استفاده کرد و لذا یا باید از روش‌های فیلتر استفاده کرد و یا به صورت ترکیبی از این دو شیوه بهره گرفت.
\\

روش‌های فیلتر خود به چندین دسته قابل تقسیم هستند؛ نخست آنکه می‌توان این روش‌ها را به روش‌های محلی\LTRfootnote{Local} و روش‌های جهانی\LTRfootnote{Global} تقسیم کرد. در روش‌های جهانی به ویژگی یک امتیاز مطلق داده می‌شود اما در روش‌های محلی به هر ویژگی متناسب با هر کلاس یک امتیاز داده می‌شود؛ بعنی در روش‌های محلی مشخص است که یک ویژگی برای هر کلاس تا چه میزان خاصیت متمایزکننده دارد. در حالتی که از یک معیار محلی استفاده می‌شود می‌توان مشخص کرد که یک ویژگی می‌توان عضویت یک متن به یک کلاس را نشان دهد یا آنکه عدم عضویت را می‌تواند به خوبی نشان دهد. اگر عضویت را بتواند بهتر نشان دهد آن را یک ویژگی مثبت\LTRfootnote{Positive} و در غیر این صورت آن را یک ویژگی منفی\LTRfootnote{Negative} برای آن کلاس به حساب می‌آورند.\cite{uysal2016improved}.
\\

روش‌های فیلتر را می‌توان به دو دسته تک متغیره\LTRfootnote{Univariate} و چندمتغیره \LTRfootnote{Multivariate} هم تقسیم کرد. در روش‌های تک متغیره هر ویژگی به صورت مستقل از سایر ویژگی‌ها امتیاز دریافت می‌کند، ولی در روش‌های چند متغیره در کنار آن که به شباهت ویژگی به هدف نگاه می‌شود، به زائد نبودن ویژگی‌ها نسبت به یکدیگر هم توجه می‌شود.\cite{labani2018novel}

\section{محاسبه روش‌های انتخاب ویژگی}
در این بخش مهم‌ترین معیار‌های انتخاب ویژگی معرفی می‌شوند و نحوه محاسبه آن‌ها ارائه می‌شود. این معیار‌ها تماما جز روش‌های انتخاب ویژگی فیلتر هستند.

\subsection{بهره اطلاعاتی}
بهره اطلاعاتی\LTRfootnote{Information Gain}
یکی از معیارهای محبوب برای انتخاب ویژگی در مقالات است
\cite{labani2018novel}\cite{uysal2016improved}.
نحوه محاسبه این معیار برای یک کلمه در رابطه ۲-۱ آمده است.

\begin{equation}
IG(t) = -\sum_{i=1}^M P(C_i)\log{P(C_i)} + P(t)\sum_{i=1}^M P(C_i|t)\log{P(C_i|t)} + P(\bar{t})\sum_{i=1}^M P(C_i|\bar{t})\log{P(C_i|\bar{t})}
\end{equation}

در این رابطه
$IG(t)$
به معنای مقدار بهره اطلاعاتی برای کلمه
$t$
است. 
$M$
  برابر با تعداد کلاس‌ها است.
$P(C_i)$
احتمال کلاس
$C_i$
است؛ یعنی چه تعدادی از اسناد به این کلاس تعلق دارند.
$P(t)$
احتمال مربوط به کلمه
$t$
است؛ یعنی آنکه چه تعدادی از اسناد شامل این کلمه هستند. به طور مشابه 
$P(\bar{t})$
به معنای احتمال عدم این کلمه است؛ یعنی آنکه چه تعدادی از اسناد شامل این کلمه نیستند.
$P(C_i|t)$
احتمال کلاس
$C_i$
به شرط کلمه
$t$
است؛ بدین معنا که چه تعدادی از اسناد شامل کلمه
$t$
به کلاس
$C_i$
تعلق دارند.
به طور مشابه
$P(C_i|\bar{t})$
هم تعریف می‌شود.

\subsection{شاخص جینی}
 شاخص جینی\LTRfootnote{Gini index}
معیاری دیگر برای انتخاب ویژگی است که در مقالاتی مورد استفاده قرار گرفته است
\cite{labani2018novel}\cite{uysal2016improved}.
نحوه محاسبه این معیار در رابطه ۲-۲ آورده شده است.

\begin{equation}
GI(t) = \sum_{i=1}^M P(t|C_i)^2 P(C_i|t)^2
\end{equation}

در این رابطه 
$GI(t)$
به معنای مقدار شاخص جینی برای کلمه
$t$
است. 
$P(t|C_i)$
احتمال شرطی کلمه 
$t$
نسبت به کلاس
$C_i$
است؛ بدین تعریف که بررسی می‌کند که چه تعداد از اسناد متعلق به کلاس 
$C_i$
دارای کلمه
$t$
هستند. سایر نماد‌های این رابطه در بخش قبل تعریف شده است.

\subsection{نسبت نابرابری}
نسبت نابرابری
\LTRfootnote{Odds Ration}
معیاری است که برای انتخاب ویژگی در مقاله اویسال
\LTRfootnote{Uysal}
استفاده شده است
\cite{uysal2016improved}
. نحوه محاسبه این معیار در رابطه ۲-۳ آورده شده است.

\begin{equation}
OR(t, C_i) = \log{\frac{P(t|C_i)[1-P(t|\bar{C_i})]}{[1-P(t|C_i)]P(t|\bar{C_i})}}
\end{equation}

در این رابطه
$OR(t, C_i)$
نسبت نابرابری به ازای کلمه
$t$
و کلاس
$C_i$
محاسبه شده است. در کار تحقیقاتی اویسال برای جلوگیری از صفر شدن مخرج مقدار 0/01 به صورت و مخرج افزوده شده است
\cite{uysal2016improved}
.

\subsection{معیار زائدی کمینه شباهت بیشینه}
معیار زائدی کمینه شباهت بیشینه
\LTRfootnote{Minimal redundancy maximal relevance}
که با نماد 
$mRMR$
یک روش انتخاب ویژگی چند متغیره است که در مقاله لبنی و همکاران مورد استفاده قرار گرفته است
\cite{labani2018novel}
. نحوه محاسبه این معیار در رابطه ۲-۴ آمده است.
\begin{equation}
mRMR(f_j) = I(f_j, C_k) - \frac{1}{|S|-1} \sum_{f_i \in S} I(f_i, f_j)
\end{equation}

در این رابطه مجموعه
$S$
به معنی مجموعه ویژگی‌های انتخابی است.
$I(a, b)$
به معنای اطلاعات متقابل
\LTRfootnote{Mutual information}
$a$
و
$b$
است.

اگر به منطق این رابطه نگاه کنیم، در می‌یابیم با این معیار به دنبال ویژگی‌های هستیم که با داده‌های یک کلاس ارتباط بالایی داشته باشند و با ویژگی‌هایی که در حال حاضر انتخاب شده‌‌اند شباهت پایین.

\subsection{معیار تمایزگر نسبی}
معیار تمایزگر نسبی\LTRfootnote{Relative discriminative criterion} یک روش انتخاب ویژگی برای مسائل دسته‌بندی دودویی است که در مقاله لبنی و همکاران\cite{labani2018novel} مورد استفاده بدوه است. نحوه محاسبه این معیار در رابطه ۲-۵ آمده است.

\begin{equation}
RDC(t, tc_i(t)) = \frac{|df_{pos}(t)-df_{neg}(t)|}{\min(df_{pos}(t),df_{neg}(t)).tc_i(t)}
\end{equation}

در این رابطه
$RDC(t, tc_i(t))$
به معنای امتیاز تمایزگر نسبی یک کلمه
$t$
و سند
$i$
-ام است.
$df_{pos}(t)$
و
$df_{neg}(t)$
به ترتیب به معنای تعداد اسناد کلاس مثبت و کلاس منفی که شامل کلمه
$t$
هستند می‌شود. منظور از
$tc_i(t)$
تعداد دفعات تکرار کلمه
$t$
در سند
$i$
-ام است. برای آنکه بتوان یک امتیاز نهایی به کلمه
$t$
نسبت داد باید تمام این امتیازها را باهم به نوعی جمع زد. مساحت زیر منحنی\LTRfootnote{Area Under the Curve(AUC)} مطابق رابطه ۲-۶ حاصل می‌شود. نهایتا
$AUC(t,tc_i)$
به ازای آخرین سند به عنوان امتیاز نهایی اعلام خواهد شد.

\begin{equation}
\begin{cases}
AUC(t,tc_1) = 0 \\
AUC(t,tc_i) = AUC(t,tc_{i-1}) + \frac{RDC(t,tc_i)+RDC(t,tc_{i+1})}{2}
\end{cases}
\end{equation}

\section{الگوریتم ژنتیک}
الگوریتم ژنتیک\LTRfootnote{Genetic algorithm} یک الگوریتم تکاملی\LTRfootnote{Evolutionary algorithm} است که با اقتباس از فرآیند تکامل موجودات زنده ارائه شده است. از آنجایی که این الگوریتم قسمت اصلی مقاله غارب و همکاران\cite{ghareb2016hybrid} را تشکیل می‌دهد، در این قسمت به صورت مختصر توضیح داده می‌شود.
\\

در الگوریتم ژنتیک ابتدا باید هر جوابی که برای مسئله وجود دارد را در قالب یک وضعیت بازنمایی کرد. در این حالت هر وضعیت نقش کرومزوم یک شخص را خواهد داشت و ژن‌های این کرومزوم مرتبط با جزئیات آن وضعیت است. سپس باید یک تعداد زیادی فرد با کرومزوم اولیه ایجاد کرد؛ چیزی که به آن جمعیت اولیه گفته می‌شود. در الگوریتم‌های ژنتیک لازم است تا یک تابع شایستگی تعریف شود. فردی که شایستگی بیشتری دارد باید مطابق با قانون تگامل شانس بیشتری برای زنده ماندن و تکثیر نسل داشته باشد. این چیزی است که در گام انتخاب والدین رخ می‌دهد. در گام انتخاب والدین، افراد با شایستگی بیشتر انتخاب خواهند شد. سپس هر دو والد دو فرزند را ایجاد می‌کنند که ژن این دو حاصل ترکیب ژن والدین است. نحوه ترکیب ژن والدین و ایجاد ژن فرزندان را بازترکیب گویند. نهایتا باید عمل جهش هم تعریف شود. در جهش برخی از ژن‌ها یک فرد تغییر می‌کند. پس از آنکه نسل جدید به وجود آمدند، نسل پیشین از بین می‌رود و الگوریتم ژنتیک با نسل جدید ادامه پیدا می‌کند تا جایی که یک شرط خاتمه برقرار شود. این شرط خاتمه می‌تواند تعداد نسل مشخص و یا همگرایی نسل‌ها باشد.
\\

در ابتدای یک الگوریتم ژنتیک عملا تعدادی جواب اولیه برای مسئله داریم و در حین الگوریتم با نسل‌های جدید، جواب‌های موجود هم بهتر می‌شود؛ چراکه یک جواب مناسب در صورتی که تابع شایستگی به خوبی تعریف شده باشد، منجر به ایجاد جواب‌های بیشتری مبتنی بر خود می‌شود و جواب‌ها نامناسب کنار گذاشته می‌شوند. نهایتا آنکه عمل بازترکیب و جهش می‌توانند تنوع جواب‌ها را حفظ کنند و به وضعیت‌هایی برسیم که در ابتدا قابل ساختن نبوده است. برای آنکه یک الگوریتم برپایه ژنتیک معرفی شود لازم است تا گام‌های گفته شده طراحی شوند؛ یعنی به عنوان مثال مشخص باشد که عمل بازترکیب چگونه رخ می‌دهد.